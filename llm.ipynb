{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "path = \"solana-whitepaper-en.pdf\"\n",
    "# Function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_file_path):\n",
    "    with open(pdf_file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Function to format the prompt for LangChain Ollama\n",
    "def format_llm_prompt(pdf_text, user_query):\n",
    "    # System instructions for the LLM\n",
    "    system_instructions = \"\"\"\n",
    "    System Instructions:\n",
    "    - You are an advanced language model trained to answer questions based on the provided context.\n",
    "    - Provide accurate, relevant, and concise responses using the given context.\n",
    "    - If the context does not provide the necessary information, state this clearly in your response.\n",
    "    \"\"\"\n",
    "\n",
    "    # Context from the PDF (truncated for length)\n",
    "    context = f\"Context: {pdf_text[:2000]}...\"  # limit context for prompt length\n",
    "\n",
    "    # Format the prompt for LangChain Ollama\n",
    "    formatted_prompt = f\"\"\"\n",
    "    <LLM Prompt>\n",
    "    <System Instructions>\n",
    "    {system_instructions}\n",
    "    \n",
    "    <Context>\n",
    "    {context}\n",
    "    \n",
    "    <User Query>\n",
    "    {user_query}\n",
    "    \"\"\"\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "def initialize_ollama_model():\n",
    "    llm = Ollama(model=\"llama2\")  # Specify the model name, e.g., \"llama2\" or any supported Ollama model.\n",
    "    return llm\n",
    "\n",
    "# Example usage\n",
    "def main(pdf_path, user_query):\n",
    "    # Step 1: Extract text from the PDF\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Step 2: Format the LLM prompt\n",
    "    formatted_prompt = format_llm_prompt(pdf_text, user_query)\n",
    "    \n",
    "    # Step 3: Initialize LangChain Ollama and run the query\n",
    "    llm = initialize_ollama_model()\n",
    "    \n",
    "    # Step 4: Use LangChain's LLM API to get a response from Ollama\n",
    "    response = llm(formatted_prompt)\n",
    "    \n",
    "    # Output the response from the LLM\n",
    "    print(\"Response from LLM:\")\n",
    "    print(response)\n",
    "\n",
    "# Example execution\n",
    "pdf_path = 'example_document.pdf'  # Replace with your PDF file path\n",
    "user_query = \"What are the key highlights of this document?\"\n",
    "\n",
    "# Run the main process\n",
    "main(pdf_path, user_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
